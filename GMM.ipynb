{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model (GMM)\n",
    "\n",
    "\n",
    "## dGMM\n",
    "\n",
    "We will derive and implement the EM algorithm for the **diagonal** Gaussian mixture model (dGMM), where all covariance matrices are constrained to be diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_dGMM(X, K, maxiter=500, tol=10**-5):\n",
    "    X = np.array(X)\n",
    "    n, d = len(X), len(X[0])\n",
    "    # initialization of pi, mu, r, S\n",
    "    pi = np.zeros(K)\n",
    "    assignment = np.random.randint(K, size=n)\n",
    "    for a in assignment:\n",
    "        pi[a] += 1\n",
    "    pi = pi / n\n",
    "    mu = np.random.rand(K, d)\n",
    "    r = np.zeros((n, K))\n",
    "    S = np.ones((K, d))     # each S_k is the diagonal vector\n",
    "    l = np.zeros(maxiter)\n",
    "\n",
    "    for iter in range(maxiter):\n",
    "        for i in range(n):\n",
    "            for k in range(K):\n",
    "                # compute responsibility\n",
    "                ex = (-1/2) * sum([(X[i][j] - mu[k][j])**2 * (1/S[k][j]) for j in range(d)])\n",
    "                r[i][k] = pi[k] * np.prod(S[k])**(-1/2) * math.exp(ex)\n",
    "        r_i = np.sum(r, axis=1)\n",
    "        r = [[r[i][k] / r_i[i] for k in range(K)] for i in range(n)]\n",
    "        # negative log-likelihood\n",
    "        l[iter] = -sum([math.log(r_i[i]) for i in range(n)])\n",
    "        if iter > 0 and abs(l[iter] - l[iter-1]) <= tol * abs(l[iter-1]):\n",
    "            break\n",
    "        r_k = np.sum(r, axis=0)\n",
    "        pi = r_k / n\n",
    "        for k in range(K):\n",
    "            S[k] = np.array([sum([r[i][k] * (X[i][j]-mu[k][j])**2 for i in range(n)]) for j in range(d)])\n",
    "            mu[k] = np.sum([r[i][k] * X[i] for i in range(n)], axis=0) / r_k[k]\n",
    "            S[k] = S[k] / r_k[k]\n",
    "    return [pi, mu, [s * np.identity(d) for s in S], l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the run-time of the algorithm, for each iteration:\n",
    "\n",
    "1. Commpute responsibility (update $r_{ij}$) takes $O(d)$, and every update has $nK$ iterations - the total run-time is $O(ndK)$.\n",
    "2. Normalize $r$ takes $O(nK)$, and compute negative log-likelihood takes $O(n)$ - the total run-time is $O(nK)$.\n",
    "3. Finding $r_{.k}$ takes $O(n)$, updating $\\pi_k$ takes $O(1)$, updating $\\mu_k$ takes $O(n)$, and updating $S_k$ takes $O(nd)$ - the total run-time is then $O(ndK)$ since it iterates from 1 to $K$.\n",
    "\n",
    "The initialization takes $O(Kd)+O(nK)\\subseteq O(ndK)$.\n",
    "Hence, the running time of the algorithm is $O(\\mathtt{maxiter}\\cdot ndK)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cGMM\n",
    "\n",
    "Now we fit $d$ GMMs (each with $K$ components) to each feature dimension $X_{i,j}$, separately. This is a component wise GMM (cGMM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_cGMM(X, K, maxiter=500, tol=10**-5):\n",
    "    n, d = len(X), len(X[0])\n",
    "    # initialization of pi, mu, r, S\n",
    "    pi = np.zeros((K, d))\n",
    "    for _ in X:\n",
    "        pi[np.random.randint(K)] += np.ones(d)\n",
    "    pi /= n\n",
    "    mu = np.random.rand(K, d)\n",
    "    r = np.zeros((d, n, K))\n",
    "    S = np.ones((K, d))     # each S_k is the diagonal vector\n",
    "    l = np.zeros(maxiter)\n",
    "\n",
    "    for iter in range(maxiter):\n",
    "        for i in range(n):\n",
    "            for k in range(K):\n",
    "                for j in range(d):\n",
    "                    # compute responsibility\n",
    "                    ex = (-1/2) * (X[i][j] - mu[k][j])**2 / S[k][j]\n",
    "                    ex = math.exp(ex)\n",
    "                    r[j][i][k] = pi[k][j] * S[k][j]**(-1/2) * ex\n",
    "        r_i = np.sum(r, axis=2)\n",
    "        r /= r_i[:,:,None]\n",
    "        # negative log-likelihood\n",
    "        l[iter] = -np.sum(np.log(r_i))\n",
    "        if iter > 0 and abs(l[iter] - l[iter-1]) <= tol * abs(l[iter]):\n",
    "            break\n",
    "        r_k = np.sum(r, axis=1)\n",
    "        for k in range(K):\n",
    "            for j in range(d):\n",
    "                pi[k][j] = r_k[j][k] / n\n",
    "                mu[k][j] = np.sum(X[:,j] * r[j,:,k]) / r_k[j][k]\n",
    "                S[k][j] = np.sum(np.power(X[:,j], 2) * r[j,:,k]) / r_k[j][k] - mu[k][j]**2\n",
    "    return [pi, mu, [np.diag(s) for s in S], l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each iteration in the algorithm:\n",
    "\n",
    "1. Computing the responsibility $r$ takes $O(ndK)$\n",
    "2. Normalizing $r$ takes $O(ndK)$ since it iterates through $r$, and finding negative log-likelhood takes $O(nd)$ - total runtime is $O(ndK)$.\n",
    "3. Finding $r_k$ takes $O(ndK)$, updating $\\pi$ takes $O(dK)$, updating $\\mu$ takes $O(ndK)$, and updating $S$ takes $O(ndK)$ - total runtime is $O(ndK)$.\n",
    "\n",
    "Since the initialization also takes $O(ndK)$, the total runtime of the algorithm is then within $O(\\mathtt{maxiter}\\cdot ndK)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "\n",
    "We apply (the adapted) dGMM algorithm [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. For each of the 10 classes (digits), we can use its (and only its) training images to estimate its (class-conditional) distribution by fitting a GMM (with $K=5$, roughly corresponding to 5 styles of writing this digit). This gives us the density estimate $p(\\mathbf x | y)$ where $\\mathbf x$ is an image (of some digit) and $y$ is the class (digit). We can now classify the test set using the Bayes classifier:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat y(\\mathbf x) = \\arg\\max_{c = 0, \\ldots, 9} ~~ \\underbrace{\\mathrm{Pr}(Y = c) \\cdot p(X = \\mathbf x | Y = c)}_{\\propto ~\\mathrm{Pr}(Y=c | X=\\mathbf x)},\n",
    "\\end{align}\n",
    "$$\n",
    "where the probabilities $\\mathrm{Pr}(Y = c)$ can be estimated using the training set, e.g. the proportion of the $c$-th class in the training set, and the **density** $p(X = \\mathbf x | Y = c)$ is estimated using GMM for each class $c$ separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"./data/\",\n",
    "    train=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "testing_data = datasets.MNIST(\n",
    "    root=\"./data/\",\n",
    "    train=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "# set up\n",
    "pca = PCA(n_components=50)\n",
    "train_data = np.reshape(training_data.data.numpy() * (1/255), (-1, 28**2))\n",
    "test_data = np.reshape(testing_data.data.numpy() * (1/255), (-1, 28**2))\n",
    "pca.fit(train_data)\n",
    "train_data = pca.transform(train_data)\n",
    "test_data = pca.transform(test_data)\n",
    "train_digit = training_data.targets.numpy()\n",
    "test_digit = testing_data.targets.numpy()\n",
    "\n",
    "train_input = [[] for _ in range(10)]\n",
    "test_input = [[] for _ in range(10)]\n",
    "for i in range(len(training_data)):\n",
    "    train_input[train_digit[i]].append(train_data[i])\n",
    "for i in range(len(testing_data)):\n",
    "    test_input[test_digit[i]].append(test_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0786\n"
     ]
    }
   ],
   "source": [
    "num_digits = 10\n",
    "K = 5\n",
    "\n",
    "# GMM training\n",
    "model = [EM_dGMM(train_input[c][:5000], K) for c in range(num_digits)]\n",
    "# GMM testing\n",
    "err = 0\n",
    "for c in range(num_digits):\n",
    "    for X in test_input[c][:1000]:\n",
    "        result = []\n",
    "        for digit in range(num_digits):\n",
    "            m = model[digit]\n",
    "            pi, mu, S = m[0], m[1], m[2]\n",
    "            pr_yc = len(train_input[digit]) / 60000\n",
    "            r = 0\n",
    "            for k in range(K):\n",
    "                ex = (-1/2) * ((X - mu[k]).T @ LA.inv(S[k]) @ (X - mu[k]))\n",
    "                r += pi[k] * LA.det(S[k])**(-1/2) * math.exp(ex)\n",
    "            result.append(pr_yc * r)\n",
    "        err += 1 if c != np.argmax(result) else 0\n",
    "    # print(f\"digit {c}: err {err}\")\n",
    "err = err / 10000\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error rate for $K=5$ on the test set is 0.0786, which is very small. Due to time concern, I only tested on $K=5$."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
