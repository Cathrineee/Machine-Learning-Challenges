{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squared hinge SVM\n",
    "\n",
    "Consider the soft-margin SVM with the **squared** hinge loss:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min_{\\mathbf w\\in \\mathbb R^d, b\\in\\mathbb R}  ~ \\frac{1}{2} \\|\\mathbf w\\|_2^2 + C\\sum_{i=1}^n \\max\\{ 1- y_i(\\mathbf w^\\top \\mathbf w_i + b) , 0 \\}^{\\textcolor{red}{2}},\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\mathbf x_i \\in \\mathbb R^d$, $y_i \\in \\{\\pm 1\\}$, and $\\|\\mathbf w\\|_2 := \\sqrt{\\sum_{j=1}^d w_j^2}$ is the Euclidean norm.\n",
    "\n",
    "\n",
    "With the set of data points and their label:\n",
    "$$\n",
    "\\{x,y\\}=\\{\n",
    "    [(1,2),-1],[(2,1),1],[(3,2),-1],[(3,1),1]\n",
    "    \\}\n",
    "$$\n",
    "using the primal equation $y_i(w^Tx_i+b)\\ge1$, we have the following:\n",
    "$$\n",
    "\\begin{cases}\n",
    "w_0+2w_1+b \\le -1 \\\\ 2w_0+w_1+b \\ge 1 \\\\ 3w_0+2w_1+b \\le -1 \\\\ 3w_0+w_1+b \\ge 1\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) - (4) gives us $w_1\\le -2$. We let $w_1=2$, then one feasible solution is $w_1=2,w_0=0,b=3$ after substituting $w_1$ into (1) and (2).\n",
    "\n",
    "Since $\\Vert w\\Vert^2=w_1^2+w_2^2\\ge w_2^2=4$, our solution above has objective value $2=\\min\\ \\frac{1}{2}\\Vert w\\Vert^2$, thus our solution is an optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent for SVM\n",
    "\n",
    "We first compute the gradient with respect to $\\mathbf w$ and $b$ for each second term in (1):\n",
    "$$\n",
    "\\begin{align}\n",
    "C (1-y_i \\hat y_i)_+^2, ~~ \\text{where} ~~ \\hat y_i = \\mathbf w^\\top \\mathbf w_i + b ~~\\text{ and }~~ (t)_+^2 :=[ \\max\\{t, 0\\}]^2.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We have $C(1-y_i\\hat{y_i})_+^2=C(1-y_i(w^Tx_i+b))_+^2$, and\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial w}&=\\begin{cases}\n",
    "-2C(1-y_i(w^Tx_i+b))y_ix_i & 1-y_i\\hat{y_i}> 0\\to y_i\\hat{y_i}< 1 \\\\ 0 & 1-y_i\\hat{y_i}\\le 0\\to y_i\\hat{y_i}\\ge 1 \n",
    "\\end{cases} \\\\\n",
    "\\frac{\\partial}{\\partial b}&=\\begin{cases}\n",
    "-2C(1-y_i(w^Tx_i+b))y_i & y_i\\hat{y_i}< 1 \\\\ 0 & y_i\\hat{y_i}\\ge 1 \n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_svm(X, Y, w, b, C, max_pass, step_size):\n",
    "    n = len(Y)\n",
    "    for _ in range(max_pass):\n",
    "        # prev_w, prev_b = w, b\n",
    "        for i in range(n):\n",
    "            y_hat = np.dot(X[i], w) + b\n",
    "            # y_hat = 1 if y_hat >= 0 else -1\n",
    "            # print(y_hat)\n",
    "            if Y[i] * y_hat <= 1:\n",
    "                d = -2 * C * (1 - Y[i] * y_hat) * Y[i]\n",
    "                # print(d)\n",
    "                w -= d * X[i] * step_size / n\n",
    "                b -= d * step_size / n\n",
    "            w = w / (1 + step_size)\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The w and b value we got from the soft-margin SVM with C = 360 is:\n",
      "w: [ 0.04797872 -2.00850051]\n",
      "b: 2.915148269391086\n"
     ]
    }
   ],
   "source": [
    "toy_x = np.array([[1,2],[2,1],[3,2],[3,1]])\n",
    "toy_y = np.array([-1,1,-1,1])\n",
    "w, b = sgd_svm(toy_x, toy_y, np.zeros(len(toy_x[0])), 0, 360, 500, 0.001)\n",
    "print(f\"The w and b value we got from the soft-margin SVM with C = 360 is:\")\n",
    "print(f\"w: {w}\")\n",
    "print(f\"b: {b}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
