{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "\n",
    "We will implement the logistic regression algorithm with the following parameters.  \n",
    "\n",
    "Let $\\Pr(C_1|x) = \\sigma({\\bf{w}}^T{\\bf{x}}+w_0)$ and $\\Pr(C_2|{\\bf{x}}) = 1 - \\sigma({\\bf{w}}^T{\\bf{x}}+w_0)$.  Learn the parameters ${\\bf{w}}$ and $w_0$ by conditional likelihood maximization. More specifically we will use Newton's algorithm to optimize the parameters and add a penalty of $0.5\\lambda ||\\bf{w}||_2^2$ to regularize the weights. Then we will find the optimal hyperparameter $\\lambda$ by 10-fold cross-validation.\n",
    "\n",
    "\n",
    "\n",
    "We know $\\sigma(\\mathbf w^T\\mathbf x+w_0)=\\frac{1}{1+\\exp\\{\\mathbf w^T\\mathbf x+w_0\\}}$, and the conditional likelihood maximization with the penalty is $\\min_w \\sum_{i=1}^n\\mathrm{lgt}(y_i\\vec{w}^T\\vec{x}_i)+0.5\\lambda\\Vert \\mathbf w\\Vert^2$, and using the Newton's algorithm, we first compute the gradient and get\n",
    "$$\n",
    "\\nabla_w \\left(\\mathrm{lgt}(y_i\\vec{w}^T\\vec{x}_i)+0.5\\lambda \\Vert\\mathbf w\\Vert^2\\right)=\\left(p(\\mathbf x_i;\\vec{w})-\\frac{y_i+1}{2}\\right)\\vec{x}_i+\\lambda\\mathbf w\n",
    "$$\n",
    "where $\\vec{w}=[\\mathbf w,w_0]^T$, and $\\vec{x}_i=[\\mathbf x_1,1]^T$. Then, the hessian is\n",
    "$$\n",
    "H_i=\\vec{x}_i[\\nabla_w p(\\mathbf x_i;\\vec{w})]^T+\\lambda I= \\sigma(\\mathbf w^T\\mathbf x+w_0)[1-\\sigma(\\mathbf w^T\\mathbf x+w_0)]\\vec{x}_i{\\vec{x}_i}^T+\\lambda I\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = pd.read_csv(\"datasets/logistic_regression-dataset/trainData1.csv\", header=None).to_numpy()\n",
    "X_2 = pd.read_csv(\"datasets/logistic_regression-dataset/trainData2.csv\", header=None).to_numpy()\n",
    "X_3 = pd.read_csv(\"datasets/logistic_regression-dataset/trainData3.csv\", header=None).to_numpy()\n",
    "X_4 = pd.read_csv(\"datasets/logistic_regression-dataset/trainData4.csv\", header=None).to_numpy()\n",
    "X_5 = pd.read_csv(\"datasets/logistic_regression-dataset/trainData5.csv\", header=None).to_numpy()\n",
    "X_6 = pd.read_csv(\"datasets/logistic_regression-dataset/trainData6.csv\", header=None).to_numpy()\n",
    "X_7 = pd.read_csv(\"datasets/logistic_regression-dataset/trainData7.csv\", header=None).to_numpy()\n",
    "X_8 = pd.read_csv(\"datasets/logistic_regression-dataset/trainData8.csv\", header=None).to_numpy()\n",
    "X_9 = pd.read_csv(\"datasets/logistic_regression-dataset/trainData9.csv\", header=None).to_numpy()\n",
    "X_10 = pd.read_csv(\"datasets/logistic_regression-dataset/trainData10.csv\", header=None).to_numpy()\n",
    "Y_1 = pd.read_csv(\"datasets/logistic_regression-dataset/trainLabels1.csv\", header=None).to_numpy()\n",
    "Y_2 = pd.read_csv(\"datasets/logistic_regression-dataset/trainLabels2.csv\", header=None).to_numpy()\n",
    "Y_3 = pd.read_csv(\"datasets/logistic_regression-dataset/trainLabels3.csv\", header=None).to_numpy()\n",
    "Y_4 = pd.read_csv(\"datasets/logistic_regression-dataset/trainLabels4.csv\", header=None).to_numpy()\n",
    "Y_5 = pd.read_csv(\"datasets/logistic_regression-dataset/trainLabels5.csv\", header=None).to_numpy()\n",
    "Y_6 = pd.read_csv(\"datasets/logistic_regression-dataset/trainLabels6.csv\", header=None).to_numpy()\n",
    "Y_7 = pd.read_csv(\"datasets/logistic_regression-dataset/trainLabels7.csv\", header=None).to_numpy()\n",
    "Y_8 = pd.read_csv(\"datasets/logistic_regression-dataset/trainLabels8.csv\", header=None).to_numpy()\n",
    "Y_9 = pd.read_csv(\"datasets/logistic_regression-dataset/trainLabels9.csv\", header=None).to_numpy()\n",
    "Y_10 = pd.read_csv(\"datasets/logistic_regression-dataset/trainLabels10.csv\", header=None).to_numpy()\n",
    "test_X = pd.read_csv(\"datasets/logistic_regression-dataset/testData.csv\", header=None).to_numpy()\n",
    "test_Y = pd.read_csv(\"datasets/logistic_regression-dataset/testLabels.csv\", header=None).to_numpy()\n",
    "\n",
    "X = [X_1,X_2,X_3,X_4,X_5,X_6,X_7,X_8,X_9,X_10]\n",
    "Y = [Y_1,Y_2,Y_3,Y_4,Y_5,Y_6,Y_7,Y_8,Y_9,Y_10]\n",
    "# scaler = StandardScaler()\n",
    "# for i in range(10):\n",
    "#     X[i] = scaler.fit_transform(X[i])\n",
    "# test_x = scaler.fit_transform(test_X)\n",
    "for i in range(10):\n",
    "    for j in range(len(Y[i])):\n",
    "        Y[i][j] = -1 if Y[i][j] == 5 else 1\n",
    "for i in range(len(test_Y)):\n",
    "    test_Y[i] = -1 if test_Y[i] == 5 else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgt(t):\n",
    "    return math.log(1 + math.exp(-1 * t))\n",
    "def sgm(t):\n",
    "    return 1 / (1 + math.exp(-1 * t))\n",
    "\n",
    "def newton_logistic(X, Y, maxIter, stepSize, lam):\n",
    "    n, d = len(Y), len(X[0])\n",
    "    w = np.matrix(np.zeros(d+1))\n",
    "    for _ in range(maxIter):\n",
    "        g, H = np.matrix(np.zeros(d+1)), np.empty([d+1,d+1])\n",
    "        H.fill(0)\n",
    "        # print(H.shape)\n",
    "        for i in range(n):\n",
    "            x = np.matrix(np.append(X[i], [1]))\n",
    "            # print(x)\n",
    "            p = sgm(np.matmul(w, x.T))\n",
    "            g += (p - (1 + Y[i]) / 2) * x + lam * w\n",
    "            H += p * (1 - p) * np.matmul(x.transpose(), x) + lam * np.identity(d+1)\n",
    "        # print(H.shape, g.shape)\n",
    "        inv = LA.solve(H, g.T)\n",
    "        # print(inv.T)\n",
    "        w -= stepSize * inv.T\n",
    "        # print(w)\n",
    "        # print(k)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "lambda_lst = range(50)\n",
    "p_lst = np.zeros(50)\n",
    "for k, lam in enumerate(lambda_lst):\n",
    "    for l in range(10):\n",
    "        train_x, train_y, test_x, test_y = np.array([]), np.array([]), np.array([]), np.array([])\n",
    "        for i in range(10):\n",
    "            if i != l:\n",
    "                train_x = np.append(train_x, X[i], axis = 0) if len(train_x != 0) else X[i]\n",
    "                train_y = np.append(train_y, Y[i], axis = 0) if len(train_y != 0) else Y[i]\n",
    "            else:\n",
    "                test_x, test_y = X[i], Y[i]\n",
    "        w = np.asarray(newton_logistic(train_x, train_y, 10, 0.1, lam)).flatten()\n",
    "        w, w_0 = w[:-1], w[-1]\n",
    "        correct, incorrect = 0, 0\n",
    "        for i in range(len(test_y)):\n",
    "            y = np.dot(w, test_x[i]) + w_0 \n",
    "            y = 1 if y >= 0 else -1\n",
    "            if y == test_y[i]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect += 1\n",
    "        p_lst[k] += correct / (correct + incorrect)\n",
    "        # print(k, l)\n",
    "    p_lst[k] /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lambda  Accuracy\n",
      "      0     0.859\n",
      "      1     0.880\n",
      "      2     0.883\n",
      "      3     0.884\n",
      "      4     0.881\n",
      "      5     0.884\n",
      "      6     0.886\n",
      "      7     0.885\n",
      "      8     0.884\n",
      "      9     0.883\n",
      "     10     0.884\n",
      "     11     0.886\n",
      "     12     0.886\n",
      "     13     0.885\n",
      "     14     0.884\n",
      "     15     0.883\n",
      "     16     0.883\n",
      "     17     0.883\n",
      "     18     0.884\n",
      "     19     0.886\n",
      "     20     0.884\n",
      "     21     0.884\n",
      "     22     0.884\n",
      "     23     0.884\n",
      "     24     0.884\n",
      "     25     0.884\n",
      "     26     0.884\n",
      "     27     0.884\n",
      "     28     0.884\n",
      "     29     0.884\n",
      "     30     0.883\n",
      "     31     0.882\n",
      "     32     0.883\n",
      "     33     0.883\n",
      "     34     0.883\n",
      "     35     0.883\n",
      "     36     0.883\n",
      "     37     0.883\n",
      "     38     0.882\n",
      "     39     0.882\n",
      "     40     0.882\n",
      "     41     0.882\n",
      "     42     0.882\n",
      "     43     0.882\n",
      "     44     0.882\n",
      "     45     0.882\n",
      "     46     0.882\n",
      "     47     0.881\n",
      "     48     0.881\n",
      "     49     0.881\n",
      "The best lambda value is [ 6 11 12 19]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cross-validation accuracy')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "report = pd.DataFrame(data={'lambda': lambda_lst, 'Accuracy': p_lst})\n",
    "lam = np.take(lambda_lst, np.where(p_lst == max(p_lst)), axis=0)\n",
    "print(report.to_string(index=False))\n",
    "print(f\"The best lambda value is {lam[0]}\")\n",
    "\n",
    "# draw a graph that shows the cross-validation accuracy of logistic regression as lambda varies\n",
    "plt.figure(1)\n",
    "plt.plot(lambda_lst, p_lst)\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"Cross-validation accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lambda  Accuracy\n",
      "      6  0.900000\n",
      "     11  0.890909\n",
      "     12  0.890909\n",
      "     19  0.900000\n"
     ]
    }
   ],
   "source": [
    "all_X = np.concatenate(X, axis = 0)\n",
    "all_Y = np.concatenate(Y, axis = 0)\n",
    "feature_w = np.zeros([len(lam[0]), len(X_1[0])+1])\n",
    "for i, l in enumerate(lam[0]):\n",
    "    # print(feature_w[i].shape)\n",
    "    feature_w[i] = np.asarray(newton_logistic(all_X, all_Y, 10, 0.1, l)).flatten()\n",
    "accuracy = np.zeros(len(feature_w))\n",
    "for k, l in enumerate(feature_w):\n",
    "    r, w = 0, 0\n",
    "    for i in range(len(test_Y)):\n",
    "        x = np.append(test_X[i], [1])\n",
    "        y = np.dot(l, x)\n",
    "        prob = sgm(y)\n",
    "        if prob >= 0.5:\n",
    "            if test_Y[i] == 1: r += 1\n",
    "            else: w += 1\n",
    "        else:\n",
    "            if test_Y[i] == -1: r += 1\n",
    "            else: w += 1\n",
    "    accuracy[k] = r / (r + w)\n",
    "report = pd.DataFrame(data={'lambda':lam[0], 'Accuracy': accuracy})\n",
    "print(report.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We find four lambdas that has the maximum accuracy.\n",
      "Lambda value: 6\n",
      "w = [ 0.00179785 -0.00181426 -0.01589582 -0.0039954  -0.0143161  -0.02973412\n",
      " -0.00574083 -0.00083558 -0.00828271 -0.0058612  -0.0201823   0.00083841\n",
      " -0.01062595 -0.02285495 -0.00243332  0.00418115 -0.0008408  -0.01030262\n",
      " -0.00498467  0.0074636  -0.00464429  0.00285369  0.00384346  0.0033522\n",
      " -0.00482153 -0.00624247  0.00064874 -0.01087162 -0.01345363 -0.00232591\n",
      "  0.00522761 -0.00055225 -0.0033689   0.00540315  0.01629094  0.00979014\n",
      "  0.00201263  0.00864601  0.0024932   0.00438504  0.00247113  0.00315333\n",
      "  0.0394552   0.02051067 -0.00040089 -0.00219243  0.00897477  0.00115822\n",
      " -0.00184894  0.007873    0.01151867  0.01126569 -0.01292794  0.00797858\n",
      "  0.01816413  0.0005114   0.00433908  0.00566246 -0.02198182 -0.01015395\n",
      "  0.00975681  0.01863661  0.00512818  0.00236447]\n",
      "w0 = -0.0006115021552480576\n",
      "---------------------------------\n",
      "Lambda value: 11\n",
      "w = [ 0.00120843 -0.00120696 -0.01371048 -0.00363028 -0.01196962 -0.02344172\n",
      " -0.00479431 -0.0001195  -0.005222   -0.00567263 -0.01573284  0.00073733\n",
      " -0.00887442 -0.01709153 -0.00236566  0.00268317  0.00013954 -0.0082165\n",
      " -0.00411833  0.0062933  -0.00408983  0.00164722  0.00235474  0.00262901\n",
      " -0.00292028 -0.00500571  0.00016804 -0.00939696 -0.01058322 -0.00294687\n",
      "  0.00361538 -0.00054378 -0.00169317  0.00439471  0.01349508  0.00794819\n",
      "  0.00187677  0.00608283  0.00138217  0.0028094   0.00152568  0.00309444\n",
      "  0.03061086  0.01654589 -0.00021381 -0.00082896  0.00887734  0.00070812\n",
      " -0.00062901  0.00469514  0.00917952  0.00922793 -0.01036627  0.00672938\n",
      "  0.01527079  0.00068206  0.00302024  0.00317629 -0.0174782  -0.00804954\n",
      "  0.00853401  0.01565638  0.00483813  0.00172844]\n",
      "w0 = -0.00040118643446816825\n",
      "---------------------------------\n",
      "Lambda value: 12\n",
      "w = [ 1.13547663e-03 -1.13413617e-03 -1.33283393e-02 -3.54009377e-03\n",
      " -1.16001492e-02 -2.25432806e-02 -4.64246067e-03 -5.80803190e-05\n",
      " -4.85994220e-03 -5.58498083e-03 -1.51078821e-02  7.17525126e-04\n",
      " -8.59055601e-03 -1.63204327e-02 -2.33428967e-03  2.49891269e-03\n",
      "  2.21853144e-04 -7.91327947e-03 -3.96977511e-03  6.10103624e-03\n",
      " -3.98368976e-03  1.51788580e-03  2.18211817e-03  2.51162108e-03\n",
      " -2.70814064e-03 -4.81950301e-03  1.26488108e-04 -9.12646238e-03\n",
      " -1.01808498e-02 -2.97808801e-03  3.41797910e-03 -5.37120548e-04\n",
      " -1.52237475e-03  4.23698834e-03  1.30570636e-02  7.66935316e-03\n",
      "  1.85651517e-03  5.76063347e-03  1.25629009e-03  2.61835552e-03\n",
      "  1.41019819e-03  3.05111788e-03  2.93661986e-02  1.59517088e-02\n",
      " -1.86841414e-04 -6.77327426e-04  8.75470868e-03  6.68286725e-04\n",
      " -5.00140851e-04  4.31975138e-03  8.85621099e-03  8.92047254e-03\n",
      " -9.98273099e-03  6.52712522e-03  1.48034707e-02  6.91964396e-04\n",
      "  2.84947264e-03  2.89704448e-03 -1.68150326e-02 -7.74758118e-03\n",
      "  8.30816686e-03  1.51746002e-02  4.75070651e-03  1.62703604e-03]\n",
      "w0 = -0.0003762187560330686\n",
      "---------------------------------\n",
      "Lambda value: 19\n",
      "w = [ 7.98954455e-04 -8.08501511e-04 -1.11310698e-02 -2.96657041e-03\n",
      " -9.56739273e-03 -1.79260624e-02 -3.80930789e-03  1.42593527e-04\n",
      " -3.25956835e-03 -4.93119867e-03 -1.19240760e-02  6.08342390e-04\n",
      " -7.02972812e-03 -1.25419707e-02 -2.09173710e-03  1.66321194e-03\n",
      "  4.76350852e-04 -6.34240897e-03 -3.14631178e-03  5.03269925e-03\n",
      " -3.35477472e-03  9.74252490e-04  1.42773755e-03  1.90164864e-03\n",
      " -1.80326022e-03 -3.84231029e-03 -2.30148120e-06 -7.55701360e-03\n",
      " -8.11863648e-03 -2.92673920e-03  2.49875931e-03 -4.84920707e-04\n",
      " -8.44619342e-04  3.39285640e-03  1.06852718e-02  6.19431374e-03\n",
      "  1.71289649e-03  4.25150957e-03  7.19320794e-04  1.75898196e-03\n",
      "  8.92889709e-04  2.70751296e-03  2.30650322e-02  1.28254657e-02\n",
      " -6.12773544e-05 -6.26260929e-05  7.76641008e-03  5.09045568e-04\n",
      " -4.20271018e-06  2.67600573e-03  7.19403117e-03  7.27979566e-03\n",
      " -7.96591816e-03  5.40415069e-03  1.22161393e-02  6.84083281e-04\n",
      "  2.04093304e-03  1.71770923e-03 -1.33697871e-02 -6.18178456e-03\n",
      "  6.98695701e-03  1.25115328e-02  4.14172687e-03  1.11194048e-03]\n",
      "w0 = -0.00026263693221609865\n"
     ]
    }
   ],
   "source": [
    "print(\"We find four lambdas that has the maximum accuracy.\")\n",
    "\n",
    "for i, l in enumerate(lam[0]):\n",
    "    print(f\"Lambda value: {l}\")\n",
    "    print(f\"w = {feature_w[i][:-1]}\")\n",
    "    print(f\"w0 = {feature_w[i][-1]}\")\n",
    "    if i < len(lam[0])-1:\n",
    "        print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to the KNN technique, which predicts the y-value by finding the k-nearest neighbour of $x$ in the training set, the logistic regression has less expressivity once the testing sets are more complicated than training set. Since it has the boundry of being a logistic function, its result will be limited. However, with KNN, we can have less boundaries, since we predict $y$ directly based on the closest neighbours of $x$ in the training set, which could give us a better result.\n",
    "\n",
    "On the other hand, since we can use a regularization term with the logistic regression while KNN doesn't, KNN could have a higer possibility of overfitting than logistic regression, which makes it less expressive in this context.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
